{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Kanawut Kaewnoparat\"\n",
    "ID = \"st122109\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 03: Logistic Regression\n",
    "\n",
    "Thus far, the problems we've encountered have been *regression* problems, in which the target $y \\in \\mathbb{R}$.\n",
    "\n",
    "Today we'll start experimenting with *classification* problems, beginning with *binary* classification problems, in which the target $y \\in \\{ 0, 1 \\}$.\n",
    "\n",
    "## Background\n",
    "\n",
    "The simplest approach to classification, applicable when the input feature vector $\\mathbf{x} \\in \\mathbb{R}^n$, is a simple generalization of what we\n",
    "do in linear regression. Recall that in linear regression, we assume that the target is drawn from a Gaussian distribution whose mean is a linear function\n",
    "of $\\mathbf{x}$:\n",
    "\n",
    "$$ y \\sim {\\cal N}(\\theta^\\top \\mathbf{x}, \\sigma^2) $$\n",
    "\n",
    "In logistic regression, similarly, we'll assume that the target is drawn from a Bernoulli distribution with parameter $p$ being the probability of\n",
    "class 1:\n",
    "\n",
    "$$ y \\sim \\text{Bernoulli}(p) $$\n",
    "\n",
    "That's fine, but how do we model the parameter $p$? How is it related to $\\mathbf{x}$?\n",
    "\n",
    "In linear regression, we assume that the mean of the Gaussian is $\\theta^\\top \\mathbf{x}$, i.e., a linear function of $\\mathbf{x}$.\n",
    "\n",
    "In logistic regression, we'll assume that $p$ is a \"squashed\" linear function of $\\mathbf{x}$, i.e.,\n",
    "\n",
    "$$ p = \\text{sigmoid}(\\theta^\\top \\mathbf{x}) = g(\\theta^\\top \\mathbf{x}) = \\frac{1}{1+e^{-\\theta^\\top \\mathbf{x}}}. $$\n",
    "\n",
    "Later, when we introduce generalized linear models, we'll see why $p$ should take this form. For now, though, we can simply note that the selection makes\n",
    "sense. Since $p$ is a discrete probability, $p$ is bounded by $0 \\le p \\le 1$. The sigmoid function $g(\\cdot)$ conveniently obeys these bounds:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def f(z):\n",
    "    return 1 / (1 + np.exp( -z ))\n",
    "\n",
    "z = np.arange(-5, 5, 0.1)\n",
    "plt.plot(z, f(z), 'b-')\n",
    "plt.xlabel('z')\n",
    "plt.ylabel('g(z)')\n",
    "plt.title('Logistic sigmoid function')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the sigmoid approaches 0 as its input approaches $-\\infty$ and approaches 1 as its input approaches $+\\infty$. If its input is 0, its value is 0.5.\n",
    "\n",
    "Again, this choice of function may seem strange at this point, but bear with it! We'll derive this function from a more general principle, the generalized\n",
    "linear model, later.\n",
    "\n",
    "OK then, we now understand that for logistic regression, the assumptions are:\n",
    "\n",
    "1. The *data* are pairs $(\\textbf{x}, y) \\in \\mathbb{R}^n \\times \\{ 0, 1 \\}$.\n",
    "1. The *hypothesis function* is $h_\\theta(\\textbf{x}) = \\frac{1}{1+e^{-\\theta^\\top \\mathbf{x}}}$.\n",
    "\n",
    "What else do we need... ? A cost function and an algorithm for minimizing that cost function!\n",
    "\n",
    "## Cost function for logistic regression\n",
    "\n",
    "You can refer to the lecture notes to see the derivation, but for this lab, let's just skip to the chase.\n",
    "With the hypothesis $h_\\theta(\\mathbf{x})$ chosen as above, the log likelihood function $\\ell(\\theta)$ can be derived as\n",
    "$$ \\ell(\\theta) = \\log {\\cal L}(\\theta) =  \\sum_{i=1}^{m}y^{(i)}\\log(h_{\\theta}(\\mathbf{x}^{(i)})) + (1 - y^{(i)})\\log(1 - (h_{\\theta}(\\mathbf{x}^{(i)})) .$$\n",
    "\n",
    "Negating the log likelihood function to obtain a loss function, we have\n",
    "\n",
    "$$ J(\\theta) = - \\sum_{i=1}^m y^{(i)}\\log h_\\theta(\\mathbf{x}^{(i)}) + (1-y^{(i)})\\log(1-h_\\theta(\\textbf{x}^{(i)})) .$$\n",
    "\n",
    "There is no closed-form solution to this problem like there is in linear regression, so we have to use gradient descent to find $\\theta$ minimizing $J(\\theta)$.\n",
    "Luckily, the function *is* convex in $\\theta$ so there is just a single global minimum, and gradient descent is guaranteed to get us there eventually if we take\n",
    "the right step size.\n",
    "\n",
    "The *stochastic* gradient of $J$, for a single observed pair $(\\mathbf{x}, y)$, turns out to be (see lecture notes)\n",
    "\n",
    "$$\\nabla_J(\\theta) = (h_\\theta(\\mathbf{x}) - y)\\mathbf{x} .$$\n",
    "\n",
    "Give some thought as to whether following this gradient to increase the loss $J$ would make a worse classifier, and vice versa!\n",
    "\n",
    "Finally, we obtain the update rule for the $j$th iteration selecting training pattern $i$:\n",
    "\n",
    "$$ \\theta^{(j+1)} \\leftarrow \\theta^{(j)} + \\alpha(y^{(i)} - h_\\theta(\\textbf{x}^{(i)}))\\textbf{x}^{(i)} .$$ \n",
    "\n",
    "Note that we can perform *batch gradient descent* simply by summing the single-pair gradient over the entire training set before taking a step,\n",
    "or *mini-batch gradient descent* by summing over a small subset of the data.\n",
    "\n",
    "## Example dataset 1: student admissions data\n",
    "\n",
    "This example is from Andrew Ng's machine learning course on Coursera.\n",
    "\n",
    "The data contain students' scores for two standardized tests and an admission decision (0 or 1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exam scores [[34.62365962 78.02469282]\n",
      " [30.28671077 43.89499752]\n",
      " [35.84740877 72.90219803]\n",
      " [60.18259939 86.3085521 ]\n",
      " [79.03273605 75.34437644]]\n",
      "-----------------------------\n",
      "Admission decision [0. 0. 0. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load student admissions data. The data file does not contain headers,\n",
    "# so we use hard coded indices for exam 1, exam2, and the admission decision.\n",
    "\n",
    "data = np.loadtxt('ex2data1.txt',delimiter = ',')\n",
    "exam1_data = data[:,0]\n",
    "exam2_data = data[:,1]\n",
    "X = np.array([exam1_data, exam2_data]).T\n",
    "y = data[:,2]\n",
    "\n",
    "# Output some sample data\n",
    "\n",
    "print('Exam scores', X[0:5,:])\n",
    "print('-----------------------------')\n",
    "print('Admission decision', y[0:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the data\n",
    "\n",
    "idx_0 = np.where(y == 0)\n",
    "idx_1 = np.where(y == 1)\n",
    "\n",
    "fig1 = plt.figure(figsize=(5, 5)) \n",
    "ax = plt.axes()\n",
    "ax.set_aspect(aspect = 'equal', adjustable = 'box')\n",
    "plt.title('Distribution')\n",
    "plt.xlabel('Exam 1')\n",
    "plt.ylabel('Exam 2')\n",
    "plt.grid(axis='both', alpha=.25)\n",
    "ax.scatter(exam1_data[idx_0], exam2_data[idx_0], s=50, c='r', marker='*', label='Not Admitted')\n",
    "ax.scatter(exam1_data[idx_1], exam2_data[idx_1], s=50, c='b', marker='o', label='Admitted')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can find good values for $\\theta$ without normalizing the data.\n",
    "We will definitely want to split the data into train and test, however..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# As usual, we fix the seed to eliminate random differences between different runs\n",
    "\n",
    "random.seed(12)\n",
    "\n",
    "# Partion data into training and test datasets\n",
    "\n",
    "m, n = X.shape\n",
    "XX = np.insert(X, 0, 1, axis=1)\n",
    "y = y.reshape(m, 1)\n",
    "idx = np.arange(0, m)\n",
    "random.shuffle(idx)\n",
    "percent_train = .6\n",
    "m_train = int(m * percent_train)\n",
    "train_idx = idx[0:m_train]\n",
    "test_idx = idx[m_train:]\n",
    "X_train = XX[train_idx,:];\n",
    "X_test = XX[test_idx,:];\n",
    "\n",
    "y_train = y[train_idx];\n",
    "y_test = y[test_idx];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important functions needed later\n",
    "\n",
    "Let's put all of our important functions here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):   \n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def h(X, theta):\n",
    "    return sigmoid(X @ theta)\n",
    "\n",
    "def grad_j(X, y, y_pred):\n",
    "    return X.T @ (y - y_pred) / X.shape[0]\n",
    "    \n",
    "def j(theta, X, y):    \n",
    "    y_pred = h(X, theta)\n",
    "    error = (-y * np.log(y_pred)) - ((1 - y) * np.log(1 - y_pred))\n",
    "    cost = sum(error) / X.shape[0]\n",
    "    grad = grad_j(X, y, y_pred)\n",
    "    return cost[0], grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize theta\n",
    "\n",
    "In any iterative algorithm, we need an initial guess. Here we'll just use zeros for all parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial theta: [[0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Initial predictions: [[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]]\n",
      "Targets: [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize our parameters, and use them to make some predictions\n",
    "\n",
    "theta_initial = np.zeros((n+1, 1))\n",
    "\n",
    "print('Initial theta:', theta_initial)\n",
    "print('Initial predictions:', h(XX, theta_initial)[0:5,:])\n",
    "print('Targets:', y[0:5,:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training function\n",
    "\n",
    "Here's a function to do batch training for `num_iters` iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, y, theta_initial, alpha, num_iters):\n",
    "    theta = theta_initial\n",
    "    j_history = []\n",
    "    for i in range(num_iters):\n",
    "        cost, grad = j(theta, X, y)\n",
    "        theta = theta + alpha * grad\n",
    "        j_history.append(cost)\n",
    "    return theta, j_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the training\n",
    "\n",
    "Here we run the training function for a million batches!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train for 1000000 iterations on full training set\n",
    "\n",
    "alpha = .0005\n",
    "num_iters = 1000000\n",
    "theta, j_history = train(X_train, y_train, theta_initial, alpha, num_iters)\n",
    "\n",
    "print(\"Theta optimized:\", theta)\n",
    "print(\"Cost with optimized theta:\", j_history[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the loss curve\n",
    "\n",
    "Next let's plot the loss curve (loss as a function of iteration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(j_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"$J(\\theta)$\")\n",
    "plt.title(\"Training cost over time with batch gradient descent (no normalization)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-lab exercise from Example 1 (Total 35 points)\n",
    "\n",
    "That took a long time, right?\n",
    "\n",
    "We'll see if we can do better. We will try the following:\n",
    "\n",
    "1. Try increasing the learning rate $\\alpha$ and starting with a better initial $\\theta$. How much does it help?\n",
    "   - Try at least 2 learning rate $\\alpha$ with 2 difference $\\theta$ (4 experiments)\n",
    "   - Do not forget to plot the loss curve to compare your results\n",
    "\n",
    "2. Better yet, try *normalizing the data* and see if the training converges better. How did it go? \n",
    "   - Be sure to plot loss curves to compare the results with unnormalized and normalized data.\n",
    "\n",
    "3. Discuss the effects of normalization, learning rate, and initial $\\theta$ in your report.\n",
    "\n",
    "Do this work in the following steps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1 (5 points)\n",
    "\n",
    "Fill in two different values for $\\alpha$ and $\\theta$.\n",
    "\n",
    "Use variable names `alpha1`, `alpha2`, `theta_initial1`, and `theta_initial2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "469661aa6773cd5305bcbca1054c82d7",
     "grade": false,
     "grade_id": "cell-98f6eb46f41c6686",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# grade task: change 'None' value to number(s) or function\n",
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()\n",
    "# declare your alphas\n",
    "alpha1 = 0.0001\n",
    "alpha2 = 0.00001\n",
    "\n",
    "# initialize thetas as you want\n",
    "theta_initial1 = np.array([0.2, 0.1, 0.03]).reshape(-1,1)\n",
    "theta_initial2 = np.array([0.5, 0.45, 0.15]).reshape(-1,1)\n",
    "\n",
    "# define your num iterations\n",
    "num_iters = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "053e98ba0c27eb56eadb89857573b428",
     "grade": true,
     "grade_id": "cell-f9ecb4ab4402c8b4",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "alpha_list = [alpha1, alpha2]\n",
    "print('alpha 1:', alpha1)\n",
    "print('alpha 2:', alpha2)\n",
    "\n",
    "theta_initial_list = [theta_initial1, theta_initial2]\n",
    "print('theta 1:', theta_initial_list[0])\n",
    "print('theta 2:', theta_initial_list[1])\n",
    "\n",
    "print('Use num iterations:', num_iters)\n",
    "\n",
    "# Test function: Do not remove\n",
    "assert alpha_list[0] is not None and alpha_list[1] is not None, \"Alpha has not been filled\"\n",
    "chk1 = isinstance(alpha_list[0], (int, float))\n",
    "chk2 = isinstance(alpha_list[1], (int, float))\n",
    "assert chk1 and chk2, \"Alpha must be number\"\n",
    "assert theta_initial_list[0] is not None and theta_initial_list[1] is not None, \"initialized theta has not been filled\"\n",
    "chk1 = isinstance(theta_initial_list[0], (list,np.ndarray))\n",
    "chk2 = isinstance(theta_initial_list[1], (list,np.ndarray))\n",
    "assert chk1 and chk2, \"Theta must be list\"\n",
    "chk1 = ((n+1, 1) == theta_initial_list[0].shape)\n",
    "chk2 = ((n+1, 1) == theta_initial_list[1].shape)\n",
    "assert chk1 and chk2, \"Theta size are incorrect\"\n",
    "assert num_iters is not None and isinstance(num_iters, int), \"num_iters must be integer\"\n",
    "print(\"success!\")\n",
    "# End Test function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2 (5 points)\n",
    "\n",
    "Fill in the code required to train your model on a particular $\\alpha$ and $\\theta$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5ea4f55cd93441770beebd895608e4e7",
     "grade": false,
     "grade_id": "cell-77a540a2a0cc2031",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# grade task: change 'None, None' value to number(s) or function\n",
    "j_history_list = []\n",
    "theta_list = []\n",
    "for alpha in alpha_list:\n",
    "    for theta_initial in theta_initial_list:\n",
    "        # YOUR CODE HERE\n",
    "#         raise NotImplementedError()\n",
    "        theta_i, j_history_i = train(X_train, y_train, theta_initial, alpha, num_iters)\n",
    "        j_history_list.append(j_history_i)\n",
    "        theta_list.append(theta_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j_history_list[2][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "85d8b32e551ac03e7e946f2c677e92d0",
     "grade": true,
     "grade_id": "cell-57627ff7e32cd714",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test function: Do not remove\n",
    "assert theta_list[0] is not None and j_history_list[0] is not None, \"No values in theta_list or j_history_list\"\n",
    "chk1 = isinstance(theta_list[0], (list,np.ndarray))\n",
    "chk2 = isinstance(j_history_list[0][0], (int, float))\n",
    "assert chk1 and chk2, \"Wrong type in theta_list or j_history_list\"\n",
    "print(\"success!\")\n",
    "# End Test function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.3 (10 points)\n",
    "\n",
    "Write code to plot loss curves for each of the sequences in `j_history_list` from the previous exercise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for alpha, theta in zip(range(len(alpha_list)), range(len(theta_initial_list))):\n",
    "    print(alpha, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3f63a65931d5f3da4e50c7cd36990e0f",
     "grade": true,
     "grade_id": "cell-33ed3657769adb04",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(np.arange(num_iters), j_history_list[0], label = f'alpha = {alpha_list[0]} & theta_initial_list = {theta_initial_list[0]}')\n",
    "plt.plot(np.arange(num_iters), j_history_list[1], label = f'alpha = {alpha_list[0]} & theta_initial_list = {theta_initial_list[1]}')\n",
    "plt.plot(np.arange(num_iters), j_history_list[2], label = f'alpha = {alpha_list[1]} & theta_initial_list = {theta_initial_list[0]}')\n",
    "plt.plot(np.arange(num_iters), j_history_list[3], label = f'alpha = {alpha_list[1]} & theta_initial_list = {theta_initial_list[1]}')\n",
    "plt.legend()\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('J loss')\n",
    "plt.show()\n",
    "# raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(num_iters), j_history_list[0], label = f'alpha = {alpha_list[0]} & theta_initial_list = {theta_initial_list[0]}')\n",
    "plt.plot(np.arange(num_iters), j_history_list[1], label = f'alpha = {alpha_list[0]} & theta_initial_list = {theta_initial_list[1]}')\n",
    "plt.plot(np.arange(num_iters), j_history_list[2], label = f'alpha = {alpha_list[1]} & theta_initial_list = {theta_initial_list[0]}')\n",
    "plt.plot(np.arange(num_iters), j_history_list[3], label = f'alpha = {alpha_list[1]} & theta_initial_list = {theta_initial_list[1]}')\n",
    "# plt.legend()\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('J loss')\n",
    "plt.ylim(0,1)\n",
    "plt.xlim(0,1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f75ff7e6c6cd980abee60df22fe5c731",
     "grade": false,
     "grade_id": "cell-59c862747f0fb871",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Exercise 1.4 (10 points)\n",
    "\n",
    "- Repeat your training, but **normalize** your data before training\n",
    "- Compare the results between normalized data and unnormalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_X_train = (X_train[: , 1:] - X_train[: , 1:].mean(axis = 0)) / X_train[:, 1:].std(axis = 0)\n",
    "scaled_X_train = np.insert(scaled_X_train, 0,1,axis = 1)\n",
    "scaled_X_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_initial_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_j_history_list = []\n",
    "scaled_theta_list = []\n",
    "for alpha in alpha_list:\n",
    "    for theta_initial in theta_initial_list:\n",
    "        scaled_theta_i, scaled_j_history_i = train(scaled_X_train, y_train, theta_initial, alpha, num_iters)\n",
    "        scaled_j_history_list.append(scaled_j_history_i)\n",
    "        scaled_theta_list.append(scaled_theta_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_theta_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(num_iters), scaled_j_history_list[0], label = f'alpha = {alpha_list[0]} & theta_initial_list = {theta_initial_list[0]}')\n",
    "plt.plot(np.arange(num_iters), scaled_j_history_list[1], label = f'alpha = {alpha_list[0]} & theta_initial_list = {theta_initial_list[1]}')\n",
    "plt.plot(np.arange(num_iters), scaled_j_history_list[2], label = f'alpha = {alpha_list[1]} & theta_initial_list = {theta_initial_list[0]}')\n",
    "plt.plot(np.arange(num_iters), scaled_j_history_list[3], label = f'alpha = {alpha_list[1]} & theta_initial_list = {theta_initial_list[1]}')\n",
    "plt.legend()\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('J loss')\n",
    "plt.show()\n",
    "# raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7b1cd115f866825c9970f827ee43d67f",
     "grade": false,
     "grade_id": "cell-43c3a360c938e4be",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Exercise 1.5 (5 points)\n",
    "\n",
    "Discuss the effects of normalization, learning rate, and initial $\\theta$ in your report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We have 4 combinations of: smaller alpha & closer starting theta, smaller alpha & further starting theta, bigger alpha & closer starting theta, and lastly bigger alpha and further starting theta\n",
    "- On the broader view, we can see that the in un-normalized data, the loss funciton drops 'exponentially' after few iterations while in normalized data the loss seems to 'steadily' declines. However, the normalized data yields far lower loss even after a few iterations. This is because the normalized data makes it easier for the model to converge the the bottom.\n",
    "- On un-normalized data, the J loss cannot get smaller after reaching around 0.6. On the contrary, 3 out of 4 models on normalized data manages to lower that J loss, with the least on at 0.45. This mean the overall performance of normalized is better than that of the unnormalized.\n",
    "- In terms of learning rate, the higher the value, the steeper the J line. This is understandable because it means for each iteration the gradient experiences more change as it 'learns to jump' with wider step\n",
    "- For the initial theta, we can see that the closer the guessed initial theta to the last output theta, the initial loss will be also smaller than that of guessed initial theta that is further away."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The logistic regression decision boundary\n",
    "\n",
    "Note that when $\\theta^\\top \\textbf{x} = 0$, we have $h_\\theta(\\textbf{x}) = 0.5$. That is, we are\n",
    "equally unsure as to whether $\\textbf{x}$ belongs to class 0 or class 1. The contour at which\n",
    "$h_\\theta(\\textbf{x}) = 0.5$ is called the classifier's *decision boundary*.\n",
    "\n",
    "We know that in the plane, the equation $$ax+by+c=0$$ is the general form of a 2D line. In our case, we have\n",
    "$$\\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 = 0$$ as our decision boundary, but clearly, this is just a 2D line\n",
    "in the plane. So when we plot $x_1$ against $x_2$, it is easy to plot the boundary line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boundary_points(X, theta):\n",
    "    v_orthogonal = np.array([[theta[1,0]],[theta[2,0]]])\n",
    "    v_ortho_length = np.sqrt(v_orthogonal.T @ v_orthogonal)\n",
    "    dist_ortho = theta[0,0] / v_ortho_length\n",
    "    v_orthogonal = v_orthogonal / v_ortho_length\n",
    "    v_parallel = np.array([[-v_orthogonal[1,0]],[v_orthogonal[0,0]]])\n",
    "    projections = X @ v_parallel\n",
    "    proj_1 = min(projections)\n",
    "    proj_2 = max(projections)\n",
    "    point_1 = proj_1 * v_parallel - dist_ortho * v_orthogonal\n",
    "    point_2 = proj_2 * v_parallel - dist_ortho * v_orthogonal\n",
    "    return point_1, point_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.array([[-11.22719851],[  0.10623818], [  0.07943241]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boundary_points(X, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure(figsize=(5,5)) \n",
    "ax = plt.axes() \n",
    "ax.set_aspect(aspect = 'equal', adjustable = 'box')\n",
    "plt.title('Logistic regression boundary')\n",
    "plt.xlabel('Exam 1')\n",
    "plt.ylabel('Exam 2')\n",
    "plt.grid(axis='both', alpha=.25)\n",
    "ax.scatter(X[:,0][idx_0], X[:,1][idx_0], s=50, c='r', marker='*', label='Not Admitted')\n",
    "ax.scatter(X[:,0][idx_1], X[:,1][idx_1], s=50, c='b', marker='o', label='Admitted')\n",
    "point_1, point_2 = boundary_points(X, theta)\n",
    "plt.plot([point_1[0,0], point_2[0,0]],[point_1[1,0], point_2[1,0]], 'g-')\n",
    "plt.legend(loc=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "You may have to adjust the above code to make it work with normalized data.\n",
    "\n",
    "### Test set performance\n",
    "\n",
    "Now let's apply the learned classifier to the test data we reserved in the beginning:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_squared(y, y_pred):\n",
    "    return 1 - np.square(y - y_pred).sum() / np.square(y - y.mean()).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_soft = h(X_test, theta)\n",
    "y_test_pred_hard = (y_test_pred_soft > 0.5).astype(int)\n",
    "\n",
    "test_rsq_soft = r_squared(y_test, y_test_pred_soft)\n",
    "test_rsq_hard = r_squared(y_test, y_test_pred_hard)\n",
    "test_acc = (y_test_pred_hard == y_test).astype(int).sum() / y_test.shape[0]\n",
    "\n",
    "print('Got test set soft R^2 %0.4f, hard R^2 %0.4f, accuracy %0.2f' % (test_rsq_soft, test_rsq_hard, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For classification, accuracy is probably the more useful measure of goodness of fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Loan prediction dataset\n",
    "\n",
    "Let's take another example dataset and see what we can do with it.\n",
    "\n",
    "This dataset is from [Kaggle](https://www.kaggle.com/altruistdelhite04/loan-prediction-problem-dataset).\n",
    "\n",
    "The data concern loan applications. It has 12 independent variables, including 5 categorical variables. The dependent variable is the decision \"Yes\" or \"No\" for extending a loan to an individual who applied.\n",
    "\n",
    "One thing we will have to do is to clean the data, by filling in missing values and converting categorical data to reals.\n",
    "We will use the Python libraries pandas and sklearn to help with the data cleaning and preparation.\n",
    "\n",
    "### Read the data and take a look at it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Pandas. You may need to run \"pip3 install pandas\" at the console if it's not already installed\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import the data\n",
    "\n",
    "data_train = pd.read_csv('train_LoanPrediction.csv')\n",
    "data_test = pd.read_csv('test_LoanPrediction.csv')\n",
    "\n",
    "# Start to explore the data\n",
    "\n",
    "print('Training data shape', data_train.shape)\n",
    "print('Test data shape', data_test.shape)\n",
    "\n",
    "print('Training data:\\n', data_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(data_train.columns).difference(set(data_test.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['Loan_Status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.isnull().sum().sort_values(ascending =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the training and test data\n",
    "\n",
    "print('Missing values for train data:\\n------------------------\\n', data_train.isnull().sum().sort_values(ascending = False))\n",
    "print('Missing values for test data \\n ------------------------\\n', data_test.isnull().sum().sort_values(ascending = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle missing values\n",
    "\n",
    "We can see from the above table that the `Married` column has 3 missing values in the training dataset and 0 missing values in the test dataset.\n",
    "Let's take a look at the distribution over the datasets then fill in the missing values in approximately the same ratio.\n",
    "\n",
    "You may be interested to look at the [documentation of the Pandas `fillna()` function](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html). It's great!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['Married'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ratio of each category value\n",
    "# Divide the missing values based on ratio\n",
    "# Fillin the missing values\n",
    "# Print the values before and after filling the missing values for confirmation\n",
    "\n",
    "print(data_train['Married'].value_counts())\n",
    "\n",
    "married = data_train['Married'].value_counts()\n",
    "print('Elements in Married variable', married.shape)\n",
    "print('Married ratio ', married[0]/sum(married.values))\n",
    "\n",
    "def fill_martial_status(data, yes_num_train, no_num_train):        \n",
    "    data['Married'].fillna('Yes', inplace = True, limit = yes_num_train)\n",
    "    data['Married'].fillna('No', inplace = True, limit = no_num_train)  \n",
    "\n",
    "fill_martial_status(data_train, 2, 1)\n",
    "print(data_train['Married'].value_counts()) \n",
    "print('Missing values for train data:\\n------------------------\\n', data_train.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the number of examples missing the `Married` attribute is 0.\n",
    "\n",
    "Let's complete the data processing based on examples given and logistic regression model on training dataset. Then we'll get the model's accuracy (goodness of fit) on the test dataset.\n",
    "\n",
    "Here is another example of filling in missing values for the `Dependents` (number of children and other dependents)\n",
    "attribute. We see that categorical values are all numeric except one value \"3+\"\n",
    "Let's create a new category value \"4\" for \"3+\" and ensure that all the data is numeric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(data_train['Dependents'].value_counts())\n",
    "dependent = data_train['Dependents'].value_counts()\n",
    "\n",
    "print('Dependent ratio 1 ', dependent['0'] / sum(dependent.values))\n",
    "print('Dependent ratio 2 ', dependent['1'] / sum(dependent.values))\n",
    "print('Dependent ratio 3 ', dependent['2'] / sum(dependent.values))\n",
    "print('Dependent ratio 3+ ', dependent['3+'] / sum(dependent.values))\n",
    "\n",
    "def fill_dependent_status(num_0_train, num_1_train, num_2_train, num_3_train, num_0_test, num_1_test, num_2_test, num_3_test):        \n",
    "    data_train['Dependents'].fillna('0', inplace=True, limit = num_0_train)\n",
    "    data_train['Dependents'].fillna('1', inplace=True, limit = num_1_train)\n",
    "    data_train['Dependents'].fillna('2', inplace=True, limit = num_2_train)\n",
    "    data_train['Dependents'].fillna('3+', inplace=True, limit = num_3_train)\n",
    "    data_test['Dependents'].fillna('0', inplace=True, limit = num_0_test)\n",
    "    data_test['Dependents'].fillna('1', inplace=True, limit = num_1_test)\n",
    "    data_test['Dependents'].fillna('2', inplace=True, limit = num_2_test)\n",
    "    data_test['Dependents'].fillna('3+', inplace=True, limit = num_3_test)\n",
    "\n",
    "fill_dependent_status(9, 2, 2, 2, 5, 2, 2, 1)\n",
    "\n",
    "print(data_train['Dependents'].value_counts())\n",
    "\n",
    "# Convert category value \"3+\" to \"4\"\n",
    "\n",
    "data_train['Dependents'].replace('3+', 4, inplace = True)\n",
    "data_test['Dependents'].replace('3+', 4, inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once missing values are filled in, you'll want to convert strings to numbers.\n",
    "\n",
    "Finally, here's an example of replacing missing values for a numeric attribute. Typically, we would use the mean of the attribute over the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['Dependents'] = data_train['Dependents'].astype('int')\n",
    "data_test['Dependents'] = data_test['Dependents'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.select_dtypes(include = 'object').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.select_dtypes(include = 'object').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['LoanAmount'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(data_train['LoanAmount'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_train['LoanAmount'].value_counts())\n",
    "\n",
    "LoanAmt = data_train['LoanAmount'].value_counts()\n",
    "\n",
    "print('mean loan amount ', np.mean(data_train[\"LoanAmount\"]))\n",
    "\n",
    "loan_amount_mean = np.mean(data_train[\"LoanAmount\"])\n",
    "\n",
    "data_train['LoanAmount'].fillna(loan_amount_mean, inplace=True, limit = 22)\n",
    "data_test['LoanAmount'].fillna(loan_amount_mean, inplace=True, limit = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_amount_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "338b0327bbf4b37bb74ca23c46b81e52",
     "grade": false,
     "grade_id": "cell-2e5822ee2e432273",
     "locked": true,
     "points": 65,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "## Take-home exercise (65 points)\n",
    "\n",
    "Using the data from Example 2 above, finish the data cleaning and\n",
    "preparation. Build a logistic regression model based on the\n",
    "cleaned dataset and report the accuracy on the test and training sets.\n",
    "\n",
    "- Set up $\\mathbf{x}$ and $y$ data (10 points)\n",
    "- Train a logistic regression model and return the values of $\\theta$ and $J$ you obtained. Find the best $\\alpha$ you can; you may find it best to normalize before training. (30 points)\n",
    "- Using the best model parameters $\\theta$ you can find, run on the test set and get the model's accuracy. (10 points)\n",
    "- Summarize what you did to find the best results in this take home exercise. (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To turn in\n",
    "\n",
    "Turn in this Jupyter notebook with your solutions to he exercises and your experiment reports,\n",
    "both for the in-lab exercise and the take-home exercise. Be sure you've discussed what\n",
    "you learned in terms of normalization and data cleaning and the results\n",
    "you obtained.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Manage missing values\n",
    "### 1.1 Credity History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.isnull().sum().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['Credit_History'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(data_train['Credit_History'].value_counts(normalize = True).iloc[0] * data_train['Credit_History'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(data_train['Credit_History'].value_counts(normalize = True).iloc[1] * data_train['Credit_History'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['Credit_History'].fillna(value = 1.0, limit = 42, inplace = True )\n",
    "data_train['Credit_History'].fillna(value = 0.0, limit = 8, inplace = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['Credit_History'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['Credit_History'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(data_train['Credit_History'].value_counts(normalize = True).iloc[0] * data_test['Credit_History'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(data_train['Credit_History'].value_counts(normalize = True).iloc[1] * data_test['Credit_History'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['Credit_History'].fillna(value = 1.0, limit =24, inplace = True )\n",
    "data_test['Credit_History'].fillna(value = 0.0, limit = 5, inplace = True )\n",
    "data_test['Credit_History'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Self_employed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_train.isnull().sum().sort_values(ascending = False))\n",
    "print(data_test.isnull().sum().sort_values(ascending = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['Self_Employed'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['Self_Employed'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['Self_Employed'].value_counts(normalize =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data_train['Self_Employed'].value_counts(normalize =True).index:\n",
    "    print(f\"limit of {i}: {round(data_train['Self_Employed'].value_counts(normalize =True).loc[i] * data_train['Self_Employed'].isnull().sum())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['Self_Employed'].fillna(value = 'No', limit = 27, inplace = True)\n",
    "data_train['Self_Employed'].fillna(value = 'Yes', limit = 5, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['Self_Employed'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['Self_Employed'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data_train['Self_Employed'].value_counts(normalize =True).index:\n",
    "    print(f\"limit of {i}: {round(data_train['Self_Employed'].value_counts(normalize =True).loc[i] * data_test['Self_Employed'].isnull().sum())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['Self_Employed'].fillna(value = 'No', limit = 20, inplace = True)\n",
    "data_test['Self_Employed'].fillna(value = 'Yes', limit = 3, inplace = True)\n",
    "data_test['Self_Employed'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3) Loan Amount Term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_train.isnull().sum().sort_values(ascending = False))\n",
    "print(data_test.isnull().sum().sort_values(ascending = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['Loan_Amount_Term'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['Loan_Amount_Term'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['Loan_Amount_Term'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['Loan_Amount_Term'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data_train['Loan_Amount_Term'].value_counts(normalize = True).index :\n",
    "    print(round(data_train['Loan_Amount_Term'].value_counts(normalize = True).loc[i] * data_train['Loan_Amount_Term'].isnull().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['Loan_Amount_Term'].fillna(value = 360.0, limit = 12, inplace = True)\n",
    "data_train['Loan_Amount_Term'].fillna(value = 180.0, limit = 2, inplace = True)\n",
    "data_train['Loan_Amount_Term'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['Loan_Amount_Term'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data_train['Loan_Amount_Term'].value_counts(normalize = True).index :\n",
    "    print(round(data_train['Loan_Amount_Term'].value_counts(normalize = True).loc[i] * data_test['Loan_Amount_Term'].isnull().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['Loan_Amount_Term'].fillna(value = 360.0, limit = 5, inplace = True)\n",
    "data_test['Loan_Amount_Term'].fillna(value = 180.0, limit = 1, inplace = True)\n",
    "data_test['Loan_Amount_Term'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4) Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_train.isnull().sum().sort_values(ascending = False))\n",
    "print(data_test.isnull().sum().sort_values(ascending = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['Gender'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data_train['Gender'].value_counts(normalize = True).index :\n",
    "    print(f\"limit for {i}: {round(data_train['Gender'].value_counts(normalize = True).loc[i] * data_train['Gender'].isnull().sum())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['Gender'].fillna(value = 'Male', limit = 11, inplace = True)\n",
    "data_train['Gender'].fillna(value = 'Female', limit = 2, inplace = True)\n",
    "data_train['Gender'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data_train['Gender'].value_counts(normalize = True).index :\n",
    "    print(f\"limit for {i}: {round(data_train['Gender'].value_counts(normalize = True).loc[i] * data_test['Gender'].isnull().sum())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['Gender'].fillna(value = 'Male', limit = 9, inplace = True)\n",
    "data_test['Gender'].fillna(value = 'Female', limit =2, inplace = True)\n",
    "data_test['Gender'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_train.isnull().sum().sort_values(ascending = False))\n",
    "print(data_test.isnull().sum().sort_values(ascending = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop the column Loan_ID as each row contains unique categorical value, not quite useful to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data_train.drop(columns = 'Loan_ID')\n",
    "data_test = data_test.drop(columns = 'Loan_ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace the dependent variable 'Loan_status' to 1-Yes and 0-No"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['Loan_Status'].replace(to_replace= 'Y', value = 1, inplace =True)\n",
    "data_train['Loan_Status'].replace(to_replace= 'N', value = 0, inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['Loan_Status'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Label Encoder on Categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.select_dtypes(include= 'object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.select_dtypes(include= 'object').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = data_train.select_dtypes(include= 'object').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in categorical_cols:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in categorical_cols:\n",
    "    data_train[col] = le.fit_transform(data_train[col])\n",
    "    data_test[col] = le.transform(data_test[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3) Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_train.iloc[:, :-1]\n",
    "y = data_train.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y , test_size = 0.3, random_state = 16) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Normalize the data on numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = ['Dependents', 'ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Loan_Amount_Term']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "std_scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_X_train = X_train\n",
    "normalized_X_test = X_test\n",
    "normalized_data_test = data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_X_train[numerical_cols] = std_scaler.fit_transform(normalized_X_train[numerical_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_X_test[numerical_cols] = std_scaler.transform(normalized_X_test[numerical_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_data_test[numerical_cols] = std_scaler.transform(normalized_data_test[numerical_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_data_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(normalized_X_train[numerical_cols], axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5) Turn pandas dataframe into numpy for easier manipulation and Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### insert an intercept at first index and set initial theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.insert(X_train, 0,1, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.insert(X_test, 0,1, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.zeros(X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iter = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = np.dot(X_train, theta)\n",
    "sigmoid = 1 / (1+ (np.exp(-1 * h)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = y_train - sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = - np.sum((y_train * np.log(sigmoid)) + ((1 - y_train) * np.log(1-sigmoid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(X_train.T, error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 10000 iterations with 0.01 learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.zeros(X_train.shape[1])\n",
    "alpha = 0.01\n",
    "num_iter = 10000\n",
    "loss_record = [100,10]\n",
    "\n",
    "\n",
    "for iter in range(num_iter+1):\n",
    "    h = np.dot(X_train, theta)\n",
    "    sigmoid = 1 / (1+ (np.exp(-1 * h)))\n",
    "    error = sigmoid - y_train\n",
    "    loss = - np.sum((y_train * np.log(sigmoid)) + ((1 - y_train) * np.log(1-sigmoid)))\n",
    "    average_loss = loss / len(X_train)\n",
    "    gradient = np.dot(X_train.T, error)\n",
    "    theta = theta - (alpha * gradient)\n",
    "    loss_record.append(loss)\n",
    "    if iter % 500 == 0:\n",
    "        print(f\"{iter}: {loss, average_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10000 iterations with 0.001 learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.zeros(X_train.shape[1])\n",
    "alpha = 0.001\n",
    "num_iter = 10000\n",
    "loss_record = [100,10]\n",
    "\n",
    "\n",
    "for iter in range(num_iter+1):\n",
    "    h = np.dot(X_train, theta)\n",
    "    sigmoid = 1 / (1+ (np.exp(-1 * h)))\n",
    "    error = sigmoid - y_train\n",
    "    loss = - np.sum((y_train * np.log(sigmoid)) + ((1 - y_train) * np.log(1-sigmoid)))\n",
    "    average_loss = loss / len(X_train)\n",
    "    gradient = np.dot(X_train.T, error)\n",
    "    theta = theta - (alpha * gradient)\n",
    "    loss_record.append(loss)\n",
    "    if iter % 500 == 0:\n",
    "        print(f\"{iter}: {loss, average_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(num_iter+1), loss_record[2:])\n",
    "plt.title('Loss without early stop with iteration of 10000')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10000 iterations with 0.0001 learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.zeros(X_train.shape[1])\n",
    "alpha = 0.0001\n",
    "num_iter = 10000\n",
    "loss_record = [100,10]\n",
    "\n",
    "\n",
    "for iter in range(num_iter+1):\n",
    "    h = np.dot(X_train, theta)\n",
    "    sigmoid = 1 / (1+ (np.exp(-1 * h)))\n",
    "    error = sigmoid - y_train\n",
    "    loss = - np.sum((y_train * np.log(sigmoid)) + ((1 - y_train) * np.log(1-sigmoid)))\n",
    "    average_loss = loss / len(X_train)\n",
    "    gradient = np.dot(X_train.T, error)\n",
    "    theta = theta - (alpha * gradient)\n",
    "    loss_record.append(loss)\n",
    "    if iter % 500 == 0:\n",
    "        print(f\"{iter}: {loss, average_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10000 iterations with 0.000001 learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.zeros(X_train.shape[1])\n",
    "alpha = 0.000001\n",
    "num_iter = 10000\n",
    "loss_record = [100,10]\n",
    "\n",
    "\n",
    "for iter in range(num_iter+1):\n",
    "    h = np.dot(X_train, theta)\n",
    "    sigmoid = 1 / (1+ (np.exp(-1 * h)))\n",
    "    error = sigmoid - y_train\n",
    "    loss = - np.sum((y_train * np.log(sigmoid)) + ((1 - y_train) * np.log(1-sigmoid)))\n",
    "    average_loss = loss / len(X_train)\n",
    "    gradient = np.dot(X_train.T, error)\n",
    "    theta = theta - (alpha * gradient)\n",
    "    loss_record.append(loss)\n",
    "    if iter % 500 == 0:\n",
    "        print(f\"{iter}: {loss, average_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using different learning rate, we see the one with smallest loss is 0.001\n",
    "### Let's set threshold to make the model early stop when the diff of loss is no longer larger than threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.zeros(X_train.shape[1])\n",
    "alpha = 0.001\n",
    "num_iter = 10000\n",
    "threshold = 0.0000001\n",
    "count = 0\n",
    "loss_record = [100,10]\n",
    "\n",
    "\n",
    "while (count < num_iter) and  (np.abs(loss_record[-1] - loss_record[-2]) > threshold)   :\n",
    "    h = np.dot(X_train, theta)\n",
    "    sigmoid = 1 / (1+ (np.exp(-1 * h)))\n",
    "    error = sigmoid - y_train\n",
    "    loss = - np.sum((y_train * np.log(sigmoid)) + ((1 - y_train) * np.log(1-sigmoid)))\n",
    "    average_loss = loss / len(X_train)\n",
    "    gradient = np.dot(X_train.T, error)\n",
    "    theta = theta - (alpha * gradient)\n",
    "    loss_record.append(loss)\n",
    "    count += 1\n",
    "\n",
    "    if count % 100 == 0:\n",
    "        print(f\"{count}: {loss, average_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(count), loss_record[2:])\n",
    "plt.title('Loss per iteration with early stop at threshold = 0.0001')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Test the model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predicted = np.dot(X_train, theta)\n",
    "train_sigmoid_predicted = 1 / (1+ np.exp(-1 * train_predicted))\n",
    "train_y_hat = np.round(train_sigmoid_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predicted = np.dot(X_test, theta)\n",
    "test_sigmoid_predicted = 1 / (1+ np.exp(-1 * test_predicted))\n",
    "test_y_hat = np.round(test_sigmoid_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true = y_train, y_pred = train_y_hat ))\n",
    "print(\"=========\")\n",
    "print(accuracy_score(y_true = y_train, y_pred = train_y_hat ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true = y_test, y_pred = test_y_hat ))\n",
    "print(\"=========\")\n",
    "print(accuracy_score(y_true = y_test, y_pred = test_y_hat ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfm_train = confusion_matrix(y_true = y_train, y_pred = train_y_hat )\n",
    "cfm_test = confusion_matrix(y_true = y_test, y_pred = test_y_hat )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(cfm_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize = (15,5))\n",
    "\n",
    "sns.heatmap(cfm_train /np.sum(cfm_train) , cmap = 'coolwarm', annot =True, fmt = \".2%\" ,ax = axes[0])\n",
    "axes[0].set_title(f'Confusion matrix of training set \\nwith accuracy score of {np.round(accuracy_score(y_true = y_train, y_pred = train_y_hat ), 2)}')\n",
    "axes[0].set_ylabel('True value')\n",
    "axes[0].set_xlabel('Predicted value')\n",
    "\n",
    "sns.heatmap(cfm_test /np.sum(cfm_test) , cmap = 'coolwarm', annot =True, fmt = \".2%\" ,ax = axes[1])\n",
    "axes[1].set_title(f'Confusion matrix of test set \\nwith accuracy score of {np.round(accuracy_score(y_true = y_test, y_pred = test_y_hat ), 2)}')\n",
    "axes[1].set_ylabel('True value')\n",
    "axes[1].set_xlabel('Predicted value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Predicted on the data_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_data_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_data_test_np = np.array(normalized_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_data_test_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_data_test_np = np.insert(normalized_data_test_np, 0,1, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_data_test_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = np.dot(normalized_data_test_np, theta)\n",
    "sigmoid_predicted = 1 / (1+ np.exp(-1 * predicted))\n",
    "y_hat = np.round(sigmoid_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8) Summary from the lab\n",
    "### On Missing Data Cleaning\n",
    "- For categorical column with missing values, it will be inserted with each possible value, according to the ratio of that particular value in the training set\n",
    "- For numerical column with missing values, it will be inserted with the mean value of that feature\n",
    "\n",
    "### On Data Preparation\n",
    "- In the dataset, categorical columns have been extracted and then label-encoded with sklearn to make them numerically interpretable\n",
    "- Then for the numerical columns, normalizaiton with StandardScaler is performed to make it easier for the model to perform\n",
    "- Actually, I had previously performed the logistic regression on un-normalized data. The error occurred in the np.log\n",
    "\n",
    "### On Training the data\n",
    "-  Using logistic regression from scratch, after iterating for 10,000 times with learning rate of 0.01, 0.001, 0.0001, and 0.000001, the smallest loss function is when then learning rate is at 0.01, yielding the loss of ~ 197.296\n",
    "- Then early stop with the threshold is implemented to reduce the computational power and time. The early stop model with the same learning rate yields almost the same loss funciton and can early stop at only after iterating 2,425 times\n",
    "\n",
    "### Performance Evaluation\n",
    "- Using sklearn.metric - classfication_report and accuracy score, accuracy score on the training data is at ~81% and ~79 on the test data\n",
    "- Using sklearn.metric - confusion matrix, we see that the most of the inaccuracy falls in the 'False Positive' category, where the model predicts the loan status of 'Yes' while it is 'No' in true value \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
