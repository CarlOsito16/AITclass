{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "08AdaBoost.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d-7RVjWFPJG"
      },
      "source": [
        "# AdaBoost (Adaptive Boosting)\n",
        "### Modify the AdaBoost scratch code in our lecture such that:\n",
        "- Notice that if err = 0, then $\\alpha$ will be undefined, thus attempt to fix this by adding some very small value to the lower term\n",
        "- Notice that sklearn version of AdaBoost has a parameter learning_rate. This is in fact the $\\frac{1}{2}$ in front of the $\\alpha$ calculation. Attempt to change this $\\frac{1}{2}$ into a parameter called eta, and try different values of it and see whether accuracy is improved. Note that sklearn default this value to 1.\n",
        "- Observe that we are actually using sklearn DecisionTreeClassifier. If we take a look at it closely, it is actually using weighted gini index, instead of weighted errors that we learn above. Attempt to write your own class of class Stump that actually uses weighted errors, instead of weighted gini index. To check whether your stump really works, it should give you still relatively the same accuracy. In addition, if you do not change y to -1, it will result in very bad accuracy. Unlike sklearn version of DecisionTree, it will STILL work even y is not change to -1 since it uses gini index\n",
        "- Put everything into a class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PzFyyt8EIWI"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZnZKzjUme8Q"
      },
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X, y = make_classification(n_samples=500, random_state=1)\n",
        "y = np.where(y==0,-1,1)  #change our y to be -1 if it is 0, otherwise 1\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACHDk3OiECQP",
        "outputId": "a3f2f2da-0a67-42bd-efa3-a65ba6bca1b3"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "m = X_train.shape[0]\n",
        "S = 20\n",
        "stump_params = {'max_depth': 1, 'max_leaf_nodes': 2}\n",
        "models = [DecisionTreeClassifier(**stump_params) for _ in range(S)]\n",
        "\n",
        "#initially, we set our weight to 1/m\n",
        "W = np.full(m, 1/m)\n",
        "\n",
        "#keep collection of a_j\n",
        "a_js = np.zeros(S)\n",
        "\n",
        "for j, model in enumerate(models):\n",
        "    \n",
        "    #train weak learner\n",
        "    model.fit(X_train, y_train, sample_weight = W)\n",
        "    \n",
        "    #compute the errors\n",
        "    yhat = model.predict(X_train) \n",
        "    err = W[(yhat != y_train)].sum()\n",
        "        \n",
        "    #compute the predictor weight a_j\n",
        "    #if predictor is doing well, a_j will be big\n",
        "    a_j = np.log ((1 - err) / err) / 2\n",
        "    a_js[j] = a_j\n",
        "    \n",
        "    #update sample weight; divide sum of W to normalize\n",
        "    W = (W * np.exp(-a_j * y_train * yhat)) \n",
        "    W = W / sum (W)\n",
        "    \n",
        "    print(j , err, a_j)\n",
        "        \n",
        "#make weighted predictions\n",
        "Hx = 0\n",
        "for i, model in enumerate(models):\n",
        "    yhat = model.predict(X_test)\n",
        "    Hx += a_js[i] * yhat\n",
        "    \n",
        "yhat = np.sign(Hx)\n",
        "\n",
        "print(classification_report(y_test, yhat))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 0.05142857142857143 1.457381605510162\n",
            "1 0.2744310575635866 0.4861280111506217\n",
            "2 0.3735369228692279 0.25853649067790896\n",
            "3 0.24459078334310394 0.5638365346081023\n",
            "4 0.28024270054897804 0.471629027844006\n",
            "5 0.361214355297757 0.28504868984885495\n",
            "6 0.3862669067140282 0.231515766733248\n",
            "7 0.33098597665868745 0.35186450683261894\n",
            "8 0.3271644484062004 0.36051900185192004\n",
            "9 0.34760257450595344 0.31479706232431687\n",
            "10 0.3228912787325592 0.370258092261616\n",
            "11 0.3147218946203221 0.3890676872414111\n",
            "12 0.3185198801302447 0.3802910942589498\n",
            "13 0.35149144323035836 0.30624491550448785\n",
            "14 0.27941500600345126 0.47368261022035557\n",
            "15 0.3171503701461901 0.3834493294414805\n",
            "16 0.31209720736718777 0.3951664184073478\n",
            "17 0.3069237308209477 0.40727038337756866\n",
            "18 0.35753653579435773 0.2930361975822687\n",
            "19 0.3093663548459018 0.40154165674894005\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.96      0.97      0.97        79\n",
            "           1       0.97      0.96      0.96        71\n",
            "\n",
            "    accuracy                           0.97       150\n",
            "   macro avg       0.97      0.97      0.97       150\n",
            "weighted avg       0.97      0.97      0.97       150\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilOay8yvEW8v",
        "outputId": "a211ede3-3dc4-48cb-c815-b3ec36572ba0"
      },
      "source": [
        "#make weighted predictions\n",
        "Hx = 0\n",
        "for i, model in enumerate(models):\n",
        "    yhat = model.predict(X_test)\n",
        "    Hx += a_js[i] * yhat\n",
        "yhat = np.sign(Hx)\n",
        "\n",
        "print(classification_report(y_test, yhat))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.96      0.97      0.97        79\n",
            "           1       0.97      0.96      0.96        71\n",
            "\n",
            "    accuracy                           0.97       150\n",
            "   macro avg       0.97      0.97      0.97       150\n",
            "weighted avg       0.97      0.97      0.97       150\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elJMVUkvk7Td"
      },
      "source": [
        "---\n",
        "# Put Everything into Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ize04ipFGyBt"
      },
      "source": [
        "class Stump():\n",
        "    def __init__(self):\n",
        "        # Determines whether threshold should be evaluated as < or >\n",
        "        self.polarity = 1\n",
        "        self.feature_index = None\n",
        "        self.threshold = None\n",
        "        # Voting power of the stump\n",
        "        self.alpha = None"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7y8vs7BgH_g8",
        "outputId": "24016fae-1305-4e66-ffc7-dfd55c96e688"
      },
      "source": [
        "np.full(shape = (3,2), fill_value= (1,2))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 2],\n",
              "       [1, 2],\n",
              "       [1, 2]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXMsEfoPGz_D"
      },
      "source": [
        "class Adaboost:\n",
        "\n",
        "\n",
        "    def __init__(self, numClassifier, eta = 0.5):\n",
        "        self.numClassifier = numClassifier\n",
        "        self.eta = eta\n",
        "\n",
        "\n",
        "\n",
        "    def fit(self, X, y):\n",
        "\n",
        "        m, n = X.shape\n",
        "        W = np.full(shape = m , fill_value=  1/m )\n",
        "\n",
        "\n",
        "        self.classifiers = []\n",
        "\n",
        "\n",
        "        for i in range(self.numClassifier):\n",
        "\n",
        "            clf = Stump()\n",
        "\n",
        "\n",
        "            #set initially minimum error to infinity\n",
        "            #so at least the first stump is identified\n",
        "            min_err = np.inf\n",
        "\n",
        "\n",
        "            for feature in range(n):\n",
        "                feature_value = np.sort(np.unique(X[:, feature]))\n",
        "                thresholds = (feature_value[:-1] + feature_value[1:]) / 2\n",
        "                # print(i , feature, len(feature_value), len(thresholds))\n",
        "                assert len(thresholds) == len(feature_value) - 1 , 'threshold number must be feature_value -1'\n",
        "\n",
        "\n",
        "                for threshold in thresholds:\n",
        "                    for polarity in [1, -1]:\n",
        "                        yhat = np.ones(len(y))\n",
        "                        yhat[polarity * X[:, feature] < polarity * threshold] = -1  #polarity=1 rule\n",
        "                        error = W[(yhat != y)].sum()\n",
        "\n",
        "                \n",
        "\n",
        "                        # print(polarity, error)\n",
        "\n",
        "                        #save the best stump\n",
        "                        if error < min_err:\n",
        "                            clf.polarity = polarity\n",
        "                            clf.threshold = threshold\n",
        "                            clf.feature_index = feature\n",
        "                            min_err = error\n",
        "\n",
        "                            # print(clf.feature_index, clf.threshold, clf.polarity, min_err)\n",
        "\n",
        "            print(clf.feature_index, clf.threshold, clf.polarity, min_err)\n",
        "            #once we know which is the best stump\n",
        "            #we calculate its alpha, and reweight samples\n",
        "            eps = 1e-10 #to prevent division by zero\n",
        "            clf.alpha = self.eta * (np.log ((1 - error) / (error + eps))) # error + eps to avoid 0 as denominator\n",
        "            W = W * np.exp(-clf.alpha * y * yhat) \n",
        "            W = W / np.sum(W)\n",
        "     \n",
        "            print(clf.alpha)\n",
        "            self.classifiers.append(clf)\n",
        "\n",
        "\n",
        "    \n",
        "    \n",
        "    def predict(self, X):\n",
        "        m, n = X.shape\n",
        "        yhat = np.zeros(m)\n",
        "        for clf in self.classifiers:\n",
        "            pred = np.ones(m) #set all to 1\n",
        "            pred[clf.polarity * X[:, clf.feature_index] < clf.polarity * clf.threshold] = -1  #polarity=1 rule\n",
        "            print(clf.alpha)\n",
        "\n",
        "            yhat += clf.alpha * pred\n",
        "\n",
        "            # print(yhat)\n",
        "\n",
        "        return np.sign(yhat)\n",
        "\n",
        "            \n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzUB9TLZJqHe"
      },
      "source": [
        "test = Adaboost(numClassifier= 5, eta = 0.5)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZtYEGugQKTGS",
        "outputId": "ffcfd07c-33c5-4d5e-e5dd-c78995054460"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(500, 20)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSls9CKZJsLt",
        "outputId": "f57d7c08-8576-4f2b-c51a-a8af399df658"
      },
      "source": [
        "test.fit(X_train,y_train)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4 0.06187989795417792 1 0.05142857142857143\n",
            "0.034299158067825825\n",
            "4 0.06187989795417792 1 0.0514890973877671\n",
            "3.550271188133721e-12\n",
            "4 0.06187989795417792 1 0.05148909738777338\n",
            "2.2204460492503126e-16\n",
            "4 0.06187989795417792 1 0.05148909738777338\n",
            "2.2204460492503126e-16\n",
            "4 0.06187989795417792 1 0.05148909738777339\n",
            "-6.661338147750944e-16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqKegeOqJyrq",
        "outputId": "1e552020-e7c3-4aab-b871-23d0c4627149"
      },
      "source": [
        "test.predict(X_test)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.034299158067825825\n",
            "3.550271188133721e-12\n",
            "2.2204460492503126e-16\n",
            "2.2204460492503126e-16\n",
            "-6.661338147750944e-16\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-1., -1., -1., -1., -1., -1., -1.,  1., -1.,  1.,  1.,  1.,  1.,\n",
              "       -1., -1.,  1.,  1., -1.,  1.,  1.,  1.,  1., -1.,  1., -1.,  1.,\n",
              "       -1., -1., -1., -1., -1.,  1., -1.,  1., -1., -1.,  1., -1., -1.,\n",
              "        1.,  1., -1., -1., -1.,  1., -1.,  1., -1.,  1.,  1.,  1.,  1.,\n",
              "        1., -1., -1., -1., -1.,  1., -1., -1., -1.,  1., -1., -1.,  1.,\n",
              "        1.,  1.,  1., -1.,  1.,  1., -1., -1., -1.,  1., -1., -1.,  1.,\n",
              "       -1.,  1.,  1.,  1.,  1.,  1., -1., -1.,  1.,  1.,  1., -1.,  1.,\n",
              "       -1., -1.,  1., -1.,  1.,  1.,  1.,  1., -1.,  1., -1.,  1., -1.,\n",
              "        1.,  1., -1.,  1., -1.,  1., -1., -1.,  1., -1., -1.,  1.,  1.,\n",
              "       -1., -1.,  1., -1.,  1., -1., -1.,  1., -1., -1.,  1., -1., -1.,\n",
              "       -1., -1., -1., -1., -1., -1., -1., -1.,  1.,  1., -1.,  1.,  1.,\n",
              "        1.,  1.,  1., -1., -1., -1.,  1.])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-yc7FPjSazs",
        "outputId": "c474dc65-48ef-4bb0-b410-3525736074e5"
      },
      "source": [
        "y_hat = test.predict(X_test)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.034299158067825825\n",
            "3.550271188133721e-12\n",
            "2.2204460492503126e-16\n",
            "2.2204460492503126e-16\n",
            "-6.661338147750944e-16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rTKZppDTY2o"
      },
      "source": [
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, confusion_matrix"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJckBizQhGVW",
        "outputId": "a6f40d15-8021-4713-a64a-6583ec663d13"
      },
      "source": [
        "print(classification_report(y_true = y_test, y_pred = yhat))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.96      0.97      0.97        79\n",
            "           1       0.97      0.96      0.96        71\n",
            "\n",
            "    accuracy                           0.97       150\n",
            "   macro avg       0.97      0.97      0.97       150\n",
            "weighted avg       0.97      0.97      0.97       150\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZufwMz6hMQj",
        "outputId": "779f0ca4-f24c-4cb2-c1e1-a09a08157011"
      },
      "source": [
        "y_test"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-1, -1, -1, -1, -1, -1, -1,  1, -1,  1,  1,  1,  1, -1, -1,  1,  1,\n",
              "       -1,  1,  1,  1,  1, -1,  1, -1,  1, -1, -1, -1, -1, -1,  1, -1, -1,\n",
              "       -1, -1,  1, -1, -1,  1, -1, -1, -1, -1,  1, -1,  1, -1,  1,  1,  1,\n",
              "        1, -1, -1, -1, -1, -1,  1, -1, -1,  1,  1, -1, -1,  1,  1,  1,  1,\n",
              "       -1,  1,  1, -1, -1, -1,  1, -1, -1,  1, -1,  1,  1,  1,  1,  1, -1,\n",
              "       -1,  1,  1,  1, -1,  1, -1,  1,  1, -1,  1,  1,  1,  1, -1,  1, -1,\n",
              "        1, -1,  1,  1, -1,  1, -1,  1, -1,  1,  1,  1, -1,  1,  1, -1, -1,\n",
              "        1, -1,  1, -1, -1,  1, -1, -1,  1, -1,  1, -1, -1, -1, -1, -1, -1,\n",
              "       -1, -1,  1,  1, -1, -1,  1,  1,  1,  1, -1, -1, -1,  1])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "1GgpiSWFlX99",
        "outputId": "9d9add04-662e-42ed-fc2e-29142bc7e40e"
      },
      "source": [
        "sns.heatmap(confusion_matrix(y_true = y_test, y_pred = yhat) / len(y_test), annot = True, cmap = 'coolwarm', fmt = '.2%')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f476d48ebd0>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SB_vDUMhnDwi"
      },
      "source": [
        "--- \n",
        "## SUMMARY\n",
        "- Adaboost with adjustable eta at 0.5 and 5 classifier gives the accuracy score at 97%"
      ]
    }
  ]
}